

from transformers import AutoTokenizer, AutoProcessor, Qwen2_5_VLForConditionalGeneration
from qwen_vl_utils import process_vision_info
import os
import json
import re
import torch

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# set up the path
image_dir = 'images'
output_json = 'Image_Caption_dataset'

# default processer
processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-3B-Instruct")
### generate 5 one-sentence long captions for each image
image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.lower().endswith(('.jpg','png','jpeg'))] # note: the method .endwith take a single string or a tuple of strings as input 

# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-VL-3B-Instruct",
    device_map="auto",
)
model.eval()

# prompts = ['Add a one-sentence caption for this image.',
#     'Describe this image in one senence with good enough details.',
#     'What is happening in this image?',
#     'Describe this image in one sentence to me as if I were a 5-yers-one',
#     'Caption this image in one sentence to include the elements of who, where, when, and doing what'] # 5 difference prompts to encourage a varation when generating captions
# The default range for the number of visual tokens per image in the model is 4-16384.
# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.
# min_pixels = 256*28*28
# max_pixels = 1280*28*28
# processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-3B-Instruct", min_pixels=min_pixels, max_pixels=max_pixels)

def message_describe(filepath):
    return {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": filepath  ,
            },
            {"type": "text", "text": "Describe this image."},
        ],
    } 


## make batches ....

# for b in range(0, len(image_files), 10):
for b in range(1):
    batch = image_files[b: b+10]
    messages = [message_describe(f) for f in batch]
    text = processor.apply_chat_template(messages, 
                                         tokenize=False, 
                                         add_generation_prompt=True)
    image_inputs, _ = process_vision_info(messages)
    
    inputs = processor(
        text=[text],
        images=image_inputs,
        padding=True,
        return_tensors="pt",
    )
    inputs = inputs.to(device)
    with torch.no_grad():
        generated_ids = model.generate(**inputs, max_new_tokens=128)
    
    generated_ids_trimmed = [
        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
    output_text = processor.batch_decode(
        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )
    print(output_text)















n_captions = 5
dataset = []
for idx,image in enumerate(image_files):
    image_path = os.path.join(image_dir,image)
    captions = []
    
    for i in range(n_captions):    
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "image": image_path,
                    },
                    {"type": "text", "text": prompts[i]},
                ],
            }
        ]
        # Preparation for inference
        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, _ = process_vision_info(messages)
        inputs = processor(
            text=[text],
            images=image_inputs,
            padding=True,
            return_tensors="pt",
        )
        inputs = inputs.to("cuda")
        # Inference: Generation of the output
        with torch.no_grad():
            generated_ids = model.generate(**inputs, max_new_tokens=128)
        
        generated_ids_trimmed = [
            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )
        
        # ensure a one-sentence output
        caption_onesentence = re.split(r'(?<=[.!?])\s',output_text[0].strip())[0]
        captions.append(caption_onesentence) 
    
    
    
    # add to dataset
    dataset.append({
        'img_id': str(idx),
        'filename': image,
        'caption': captions
    })

# save out
with open(output_json, 'w') as f:
    json.dump(dataset, f, indent=2)

print(f'generated dataset saved to {output_json}')
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.16s/it]
Some parameters are on the meta device because they were offloaded to the cpu and disk.
Traceback (most recent call last):

  Cell In[15], line 69
    generated_ids = model.generate(**inputs, max_new_tokens=128)

  File ~\anaconda3\envs\mlx\Lib\site-packages\torch\utils\_contextlib.py:116 in decorate_context
    return func(*args, **kwargs)

  File ~\anaconda3\envs\mlx\Lib\site-packages\transformers\generation\utils.py:2623 in generate
    result = self._sample(

  File ~\anaconda3\envs\mlx\Lib\site-packages\transformers\generation\utils.py:3604 in _sample
    outputs = self(**model_inputs, return_dict=True)

  File ~\anaconda3\envs\mlx\Lib\site-packages\torch\nn\modules\module.py:1736 in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)

  File ~\anaconda3\envs\mlx\Lib\site-packages\torch\nn\modules\module.py:1747 in _call_impl
    return forward_call(*args, **kwargs)

  File ~\anaconda3\envs\mlx\Lib\site-packages\accelerate\hooks.py:175 in new_forward
    output = module._old_forward(*args, **kwargs)

  File ~\anaconda3\envs\mlx\Lib\site-packages\transformers\utils\generic.py:943 in wrapper
    output = func(self, *args, **kwargs)

  File ~\anaconda3\envs\mlx\Lib\site-packages\transformers\models\qwen2_5_vl\modeling_qwen2_5_vl.py:1487 in forward
    outputs = self.model(

  File ~\anaconda3\envs\mlx\Lib\site-packages\torch\nn\modules\module.py:1736 in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)

  File ~\anaconda3\envs\mlx\Lib\site-packages\torch\nn\modules\module.py:1747 in _call_impl
    return forward_call(*args, **kwargs)

  File ~\anaconda3\envs\mlx\Lib\site-packages\transformers\models\qwen2_5_vl\modeling_qwen2_5_vl.py:1228 in forward
    image_embeds = self.get_image_features(pixel_values, image_grid_thw)

  File ~\anaconda3\envs\mlx\Lib\site-packages\transformers\models\qwen2_5_vl\modeling_qwen2_5_vl.py:1178 in get_image_features
    image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)

  File ~\anaconda3\envs\mlx\Lib\site-packages\torch\nn\modules\module.py:1736 in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)

  File ~\anaconda3\envs\mlx\Lib\site-packages\torch\nn\modules\module.py:1747 in _call_impl
    return forward_call(*args, **kwargs)

  File ~\anaconda3\envs\mlx\Lib\site-packages\transformers\models\qwen2_5_vl\modeling_qwen2_5_vl.py:479 in forward
    hidden_states = self.merger(hidden_states)

  File ~\anaconda3\envs\mlx\Lib\site-packages\torch\nn\modules\module.py:1736 in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)

  File ~\anaconda3\envs\mlx\Lib\site-packages\torch\nn\modules\module.py:1747 in _call_impl
    return forward_call(*args, **kwargs)

  File ~\anaconda3\envs\mlx\Lib\site-packages\transformers\models\qwen2_5_vl\modeling_qwen2_5_vl.py:135 in forward
    x = self.mlp(self.ln_q(x).view(-1, self.hidden_size))

  File ~\anaconda3\envs\mlx\Lib\site-packages\torch\nn\modules\module.py:1736 in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)

  File ~\anaconda3\envs\mlx\Lib\site-packages\torch\nn\modules\module.py:1747 in _call_impl
    return forward_call(*args, **kwargs)

  File ~\anaconda3\envs\mlx\Lib\site-packages\accelerate\hooks.py:175 in new_forward
    output = module._old_forward(*args, **kwargs)

  File ~\anaconda3\envs\mlx\Lib\site-packages\torch\nn\modules\container.py:250 in forward
    input = module(input)

  File ~\anaconda3\envs\mlx\Lib\site-packages\torch\nn\modules\module.py:1736 in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)

  File ~\anaconda3\envs\mlx\Lib\site-packages\torch\nn\modules\module.py:1747 in _call_impl
    return forward_call(*args, **kwargs)

  File ~\anaconda3\envs\mlx\Lib\site-packages\accelerate\hooks.py:175 in new_forward
    output = module._old_forward(*args, **kwargs)

  File ~\anaconda3\envs\mlx\Lib\site-packages\torch\nn\modules\linear.py:125 in forward
    return F.linear(input, self.weight, self.bias)

RuntimeError: mat1 and mat2 shapes cannot be multiplied (2577x5120 and 2048x151936)